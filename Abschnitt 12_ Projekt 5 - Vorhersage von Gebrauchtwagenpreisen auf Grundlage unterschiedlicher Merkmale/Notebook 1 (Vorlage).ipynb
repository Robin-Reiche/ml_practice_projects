{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Daten importieren\n","*Datenquelle: [https://www.kaggle.com/datasets/adhurimquku/ford-car-price-prediction/data](https://www.kaggle.com/datasets/adhurimquku/ford-car-price-prediction/data)*"],"metadata":{"id":"tPQg-Ot21wql"}},{"cell_type":"code","source":["import pandas as pd\n","\n","# Schritt 1: Lese die entsprechende Datei ein, speichere die Daten in der Variable 'df' ab und gib sie anschließend aus.\n","df = ...\n","df"],"metadata":{"id":"eXEM5f0510Fn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Schritt 2: Analysiere die Daten mithilfe von '.describe()'\n","\n","..."],"metadata":{"id":"1jVMDX4MrteB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Daten aufteilen"],"metadata":{"id":"5l8z89BRaBN8"}},{"cell_type":"code","source":["# Schritt 3: Weise die entsprechenden Features der Eingabevariable X und das zugehörige Ziel der Zielvariable y zu.\n","# Tipp: Verwende '.copy()' und '.pop()'.\n","\n","X = ...\n","y = ..."],"metadata":{"id":"NSV5gioTaBN9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X"],"metadata":{"id":"CPSX3bdpaBN9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y"],"metadata":{"id":"ZTpasFqHYqT9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install scikit-learn==1.5.2\n","from sklearn.model_selection import train_test_split\n","\n","#Schritt 4: Unterteile die Daten in Trainings- und Testdaten, wobei die Testdaten 20% der Gesamtdaten ausmachen sollten.\n","X_train, X_test, y_train, y_test = ..."],"metadata":{"id":"cm-zVwYNaBN9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train"],"metadata":{"id":"Gdnl1xcGaBN9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_train"],"metadata":{"id":"QYj-WXzwaBN9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Random Forest Regressor - Hyperparameteroptimierung mit GridSearchCV\n"],"metadata":{"id":"ubLFPCBKj3dI"}},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestRegressor\n","from sklearn.model_selection import GridSearchCV\n","\n","# Schritt 5: Erstelle eine Instanz eines RandomForestRegressors und erstelle jeweils eine Liste mit verschiedenen Werten für 'n_estimators' und 'max_depth'.\n","rf_model = ...\n","param_grid = {\n","    'n_estimators': [...],\n","    'max_depth': [...]\n","}\n","\n","# Schritt 6: Setze bei GridSearchCV die Anzahl an Folds auf 4 und scoring auf \"neg_mean_absolute_error\".\n","grid_search = GridSearchCV(rf_model, param_grid, cv=..., scoring=...)\n","grid_search.fit(X_train, y_train)\n","\n","print(\"Best Mean Absolute Error: \", -grid_search.best_score_)\n","print(\"Best Parameters: \", grid_search.best_params_)"],"metadata":{"id":"z5lMvTQTh9bb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Random Forest Regressor - Finales Modell"],"metadata":{"id":"NqKk3oEnkokL"}},{"cell_type":"code","source":["from sklearn.metrics import mean_absolute_error\n","\n","# Schritt 7: Speichere den besten RandomForestRegressor in 'final_rf_model' ab mithilfe von '.best_estimator_' und berechne den finalen MAE auf den Testdaten.\n","final_rf_model = ...\n","y_predict_final = ...\n","mae = ...\n","\n","print(\"Final MAE:\", mae)"],"metadata":{"id":"hYmZvuY8ko-z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# XGBoost - Hyperparameteroptimierung mit GridSearchCV"],"metadata":{"id":"trDhTeXtKWvF"}},{"cell_type":"code","source":["from xgboost import XGBRegressor\n","\n","# Schritt 8: Erstelle eine Instanz eines XGBRegressors und erstelle eine Liste mit verschiedenen Werten für 'max_depth'.\n","xgb_model = ...\n","param_grid = {\n","    'max_depth': [...]\n","}\n","\n","# Schritt 9: Erstelle eine Instanz von GridSearchCV und setze die Anzahl an Folds auf 4 und scoring auf \"neg_mean_absolute_error\".\n","grid_search = ...\n","grid_search.fit(X_train, y_train)\n","\n","print(\"Best Mean Absolute Error: \", -grid_search.best_score_)\n","print(\"Best Parameter: \", grid_search.best_params_)"],"metadata":{"id":"F32JsObBKcJ8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Early Stopping, um *n_estimators* besser zu bestimmen"],"metadata":{"id":"0t5M8DN5Nt6R"}},{"cell_type":"code","source":["# Schritt 10: Teile die Trainingsdaten wieder in neue Trainings- und Validierungsdaten auf, wobei die Validierungsdaten 20% der Trainingsdaten ausmachen sollten.\n","X_train_new, X_valid, y_train_new, y_valid = ...\n","\n","# Schritt 11: Erstelle eine Instanz von XGBRegressor und setze 'n_estimators' auf einen extrem hohen Wert und 'max_depth' auf den optimalen Wert, der zuvor ermittelt wurde.\n","es_xgb_model = XGBRegressor(n_estimators=..., max_depth=..., eval_metric=\"mae\", early_stopping_rounds=70, random_state=0)\n","es_xgb_model.fit(X_train_new, y_train_new,\n","             eval_set=[(X_train_new, y_train_new), (X_valid, y_valid)],\n","             verbose=False)"],"metadata":{"id":"t7wQ_6LrL0Ur"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Besten Wert für *n_estimators* ausgeben:"],"metadata":{"id":"NCPr00WkPQ_c"}},{"cell_type":"code","source":["# Schritt 12: Gib den besten Wert für 'n_estimators' aus.\n","# Tipp: Verwende '.best_iteration'.\n","\n","print(\"Best value for n_estimators: \", ...)"],"metadata":{"id":"bm5TWiwNPL8J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Ergebnisse visualisieren"],"metadata":{"id":"Hzc1rw2lS3r1"}},{"cell_type":"code","source":["# Schritt 13: Speichere die Evaluierungsfehler von EarlyStopping in results ab.\n","# Tipp: Verwende '.evals_result()'\n","\n","results = ...\n","errors_df = pd.DataFrame({\n","    \"Train Error\": results[\"validation_0\"][\"mae\"],\n","    \"Valid Error\": results[\"validation_1\"][\"mae\"],\n","})\n","errors_df"],"metadata":{"id":"4oaRyIgXPTpy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Schritt 14: Visualisiere die Evaluierungsfehler mithilfe von seaborn.\n","sns.scatterplot(...)\n","plt.xlabel(\"Iterations\")\n","plt.ylabel(\"MAE\")"],"metadata":{"id":"CemXJB4VUXwz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# XGBoost Regressor - Finales Modell"],"metadata":{"id":"P9Y9Ut9-U-pT"}},{"cell_type":"code","source":["# Schritt 15: Erstelle eine finale Instanz von XGBRegressor mit den optimalen Werten für 'n_estimators' und 'max_depth', die zuvor ermittelt wurden.\n","\n","final_xgb_model = XGBRegressor(n_estimators=..., max_depth=..., random_state=0)"],"metadata":{"id":"JoAzAFHAU7zD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Schritt 16: Trainiere das finale Modell ein letztes Mal auf allen Trainingsdaten und berechne den MAE auf den Testdaten.\n","\n","final_xgb_model.fit(..., ...)\n","y_predict_final = ...\n","mae = ...\n","\n","print(\"Final MAE:\", mae)"],"metadata":{"id":"zD4Gj9D5XIYV"},"execution_count":null,"outputs":[]}]}