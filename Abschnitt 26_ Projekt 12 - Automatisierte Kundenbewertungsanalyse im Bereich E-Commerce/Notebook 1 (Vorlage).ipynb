{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Daten importieren\n","*Datenquelle: [https://www.kaggle.com/datasets/nicapotato/womens-ecommerce-clothing-reviews](https://www.kaggle.com/datasets/nicapotato/womens-ecommerce-clothing-reviews)*"],"metadata":{"id":"tPQg-Ot21wql"}},{"cell_type":"code","source":["import pandas as pd\n","\n","# Schritt 1: Lese die entsprechende Datei ein, speichere die Daten in der Variable 'df' ab und gib sie anschließend aus.\n","df = ...\n","df"],"metadata":{"id":"eXEM5f0510Fn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Preprocessing-Funktion definieren\n"],"metadata":{"id":"x5qT-xko9Nm_"}},{"cell_type":"code","source":["import nltk\n","nltk.download(\"tagsets\")\n","\n","# Schritt 2: Führe diese Zelle aus, um herauszufinden, mit welchen Buchstaben die Tags für Nomen und Adverbien beginnen.\n","nltk.help.upenn_tagset()"],"metadata":{"id":"XEaZuoD68Y1S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import wordnet as wn\n","\n","nltk.download(\"punkt\")\n","nltk.download(\"averaged_perceptron_tagger\")\n","nltk.download(\"wordnet\")\n","\n","# Schritt 3: Nutze die Erkenntnis aus Schritt 2, um die Funktion 'preprocess_review' zusätzlich um Nomen und Adverbien zu erweitern.\n","def preprocess_review(text):\n","  tokens = nltk.word_tokenize(text)\n","  lowercase_tokens = [token.lower() for token in tokens]\n","  pos_tags = nltk.pos_tag(lowercase_tokens)\n","  lemmatizer = WordNetLemmatizer()\n","  lemmatized_words = []\n","  for token, pos_tag in pos_tags:\n","    # Fall 1: Adjektive\n","    if pos_tag.startswith('J'):\n","      lemma = lemmatizer.lemmatize(token, pos=wn.ADJ)\n","      lemmatized_words.append(lemma)\n","    # Fall 2: Nomen\n","    elif pos_tag.startswith(...):\n","      lemma = lemmatizer.lemmatize(token, pos=wn.NOUN)\n","      lemmatized_words.append(lemma)\n","    # Fall 3: Adverbien\n","    elif pos_tag.startswith(...):\n","      lemma = lemmatizer.lemmatize(token, pos=wn.ADV)\n","      lemmatized_words.append(lemma)\n","  return ' '.join(lemmatized_words)\n","\n","# Anmerkung: Obwohl es möglich ist, diese Funktion in Python kompakter zu gestalten, habe ich mich bewusst für eine einfache Implementierung entschieden, um die Aufgabenstellung klar und verständlich zu halten."],"metadata":{"id":"GA6InbQj_k8o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Schritt 4: Verwende die Funktion, die im vorherigen Schritt definiert wurde, um die Kundenbewertungen zu verarbeiten.\n","preprocessed_review_text = df[...].apply(...)\n","preprocessed_review_text"],"metadata":{"id":"RfhFLHiLA4FO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","# Schritt 5: Definiere die Zielvariable 'y' entsprechend und führe diese Zelle aus, um die Daten in Trainings- und Testdaten aufzuteilen.\n","y = ...\n","X_train, X_test, y_train, y_test = train_test_split(preprocessed_review_text, y, test_size=0.2, stratify=y, random_state=0)"],"metadata":{"id":"Gd50pgjZ2MxG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","# Schritt 6: Erstelle eine Instanz von 'CountVectorizer' und setze 'max_features' auf 100.\n","count_vectorizer = ...\n","count_vectorizer.fit(X_train)\n","\n","# Schritt 7: Transformiere 'X_train' und 'X_test' mithilfe von 'count_vectorizer'.\n","X_train_vectorized = count_vectorizer.transform(...)\n","X_test_vectorized = count_vectorizer.transform(...)"],"metadata":{"id":"5w2MafcVL974"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Schritt 8: Führe diese Zelle aus, um die Daten aus Schritt 7 in ein DataFrame umzuwandeln.\n","feature_names = count_vectorizer.get_feature_names_out()\n","X_train_df = pd.DataFrame(X_train_vectorized.toarray(), columns=feature_names)\n","X_test_df = pd.DataFrame(X_test_vectorized.toarray(), columns=feature_names)\n","\n","X_train_df"],"metadata":{"id":"L0_kOQLk7bSU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Multinomial Naive Bayes - Modell trainieren\n"],"metadata":{"id":"X1n1tnDztAUn"}},{"cell_type":"code","source":["from sklearn.naive_bayes import MultinomialNB\n","\n","# Schritt 9: Erstelle eine Instanz von 'MultinomialNB', und trainiere das Modell entsprechend.\n","model = ...\n","model.fit(..., ...)"],"metadata":{"id":"FvYmLsYttHLH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Finale Accuracy"],"metadata":{"id":"M_mvJdPdIcA1"}},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score\n","\n","# Schritt 10: Generiere die Vorhersagen für 'X_test_df' und berechne damit die finale Accuracy basierend auf 'y_test' und 'y_predict'.\n","y_predict = model.predict(...)\n","accuracy = accuracy_score(..., ...)\n","\n","print(\"Final Accuracy:\", accuracy)"],"metadata":{"id":"pL0n6GnjId_E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Confusion Matrix"],"metadata":{"id":"_ABoIWgXZbdr"}},{"cell_type":"code","source":["from sklearn.metrics import ConfusionMatrixDisplay\n","\n","# Schritt 11: Erstelle eine Confusion Matrix basierend auf 'y_test_' und 'y_predict', um detailliert zu analysieren, welche Fehler das Modell gemacht hat.\n","ConfusionMatrixDisplay.from_predictions(..., ...)"],"metadata":{"id":"CqbShClBjb4V"},"execution_count":null,"outputs":[]}]}